{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom Gradient Descent với Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanap5/ColabNote2/blob/master/Custom_Gradient_Descent_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQoQRMbQWHw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNGp08uDWlQk",
        "colab_type": "text"
      },
      "source": [
        "## Thực hiện Gradient Decent với thư viện Tensorflow không sử dụng Built-in optimizer\n",
        "\n",
        "Một điểm thú vị trên thư viện Tensorflow đó là nó có khả năng tự động tính gradient của một hàm bất kỳ. Tất cả những gì chúng ta phải làm chỉ là setup function đó và chạy lệnh tf.gradients để tính toán gradient của hàm đó. Chúng ta không cần quan tâm quá trình tính toán gồm những bước nào từ việc tính toán lossfunction. Và hôm nay chúng ta sẽ tìm hiểu về vấn đề này.\n",
        "\n",
        "Hãy nhìn vào vài ví dụ sau để thấy rõ được vấn đề.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnJaxUaNYWSC",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Sử dụng Softmax Regression trên MNIST dataset sử dụng Optimizer tích hợp sẵn trong tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyJUFKxxZDHE",
        "colab_type": "text"
      },
      "source": [
        "Trước khi đi sâu vào tìm hiểu tf.Gradients, chúng ta sẽ bắt đầu với ví dụ giair bài toán MNIST sử dụng Optimizer có sẵn trong thư viện Tensorflow (Gradient descent optimization algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opqu-Wz6X6hQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "30ea8417-427d-40a9-8868-11083210737e"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# MNIST dataset\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
        "\n",
        "# Parameter\n",
        "learning_rate = 0.01\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "display_step = 1\n",
        "\n",
        "# tf graph input\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28 = 784\n",
        "y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
        "\n",
        "# set model weight\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# construct model\n",
        "pred = tf.nn.softmax(tf.matmul(x, W)+ b) # softmax\n",
        "\n",
        "# minimize error using cross entropy\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            # Fit training using batch data\n",
        "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys})\n",
        "#             print(__w)\n",
        "            \n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if (epoch+1) % display_step == 0:\n",
        "#             print(sess.run(W))\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "\n",
        "    print (\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy for 3000 examples\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 03:00:12.892016 140114936362880 deprecation.py:323] From <ipython-input-2-8e298ed8ff5a>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0717 03:00:12.893603 140114936362880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0717 03:00:12.894768 140114936362880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0717 03:00:12.970621 140114936362880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0717 03:00:13.236477 140114936362880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0717 03:00:13.239918 140114936362880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0717 03:00:13.316047 140114936362880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 cost= 1.183742214\n",
            "Epoch: 0002 cost= 0.665294645\n",
            "Epoch: 0003 cost= 0.552712628\n",
            "Epoch: 0004 cost= 0.498699845\n",
            "Epoch: 0005 cost= 0.465540914\n",
            "Epoch: 0006 cost= 0.442628153\n",
            "Epoch: 0007 cost= 0.425437523\n",
            "Epoch: 0008 cost= 0.412244748\n",
            "Epoch: 0009 cost= 0.401447279\n",
            "Epoch: 0010 cost= 0.392452632\n",
            "Optimization Finished!\n",
            "Accuracy: 0.87266666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsgzkB5T371a",
        "colab_type": "text"
      },
      "source": [
        "### 1. 2 Softmax Regression trên data MNIST using tf.gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Uy50o64Iwn",
        "colab_type": "text"
      },
      "source": [
        "Để thực hiện Gradient Descent chúng ta sẽ bỏ phần code liên quan đến Optimizer ở trên. Thay vào đó chúng ta sẽ tự viết phần update weight cho netwwork như sau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLJ-N6zt5asa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7429e74c-3f8a-4eee-8923-cc6546ee2a12"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 10\n",
        "batch_size = 100\n",
        "display_step = 1\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 100\n",
        "batch_size = 10\n",
        "display_step = 1\n",
        "\n",
        "# tf Graph Input\n",
        "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
        "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
        "\n",
        "# Set model weights\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Construct model\n",
        "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
        "\n",
        "# Minimize error using cross entropy\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
        "\n",
        "grad_W, grad_b = tf.gradients(xs=[W, b], ys=cost)\n",
        "\n",
        "\n",
        "new_W = W.assign(W - learning_rate * grad_W)\n",
        "new_b = b.assign(b - learning_rate * grad_b)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            # Fit training using batch data\n",
        "            _, _,  c = sess.run([new_W, new_b ,cost], feed_dict={x: batch_xs,\n",
        "                                                      y: batch_ys})\n",
        "            \n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if (epoch+1) % display_step == 0:\n",
        "#             print(sess.run(W))\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "            \n",
        "\n",
        "    print (\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy for 3000 examples\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 cost= 0.549140360\n",
            "Epoch: 0002 cost= 0.366004887\n",
            "Epoch: 0003 cost= 0.336440942\n",
            "Epoch: 0004 cost= 0.321183545\n",
            "Epoch: 0005 cost= 0.311251386\n",
            "Epoch: 0006 cost= 0.304264489\n",
            "Epoch: 0007 cost= 0.298515269\n",
            "Epoch: 0008 cost= 0.294186639\n",
            "Epoch: 0009 cost= 0.290741280\n",
            "Epoch: 0010 cost= 0.287484975\n",
            "Epoch: 0011 cost= 0.284897734\n",
            "Epoch: 0012 cost= 0.282543033\n",
            "Epoch: 0013 cost= 0.280638179\n",
            "Epoch: 0014 cost= 0.278797283\n",
            "Epoch: 0015 cost= 0.277104856\n",
            "Epoch: 0016 cost= 0.275465670\n",
            "Epoch: 0017 cost= 0.274080394\n",
            "Epoch: 0018 cost= 0.272911114\n",
            "Epoch: 0019 cost= 0.271534524\n",
            "Epoch: 0020 cost= 0.270761878\n",
            "Epoch: 0021 cost= 0.269505150\n",
            "Epoch: 0022 cost= 0.268630186\n",
            "Epoch: 0023 cost= 0.267730661\n",
            "Epoch: 0024 cost= 0.266859166\n",
            "Epoch: 0025 cost= 0.266102107\n",
            "Epoch: 0026 cost= 0.265243174\n",
            "Epoch: 0027 cost= 0.264610580\n",
            "Epoch: 0028 cost= 0.263875189\n",
            "Epoch: 0029 cost= 0.263324106\n",
            "Epoch: 0030 cost= 0.262513349\n",
            "Epoch: 0031 cost= 0.261973620\n",
            "Epoch: 0032 cost= 0.261368689\n",
            "Epoch: 0033 cost= 0.260856389\n",
            "Epoch: 0034 cost= 0.260268418\n",
            "Epoch: 0035 cost= 0.259985448\n",
            "Epoch: 0036 cost= 0.259165047\n",
            "Epoch: 0037 cost= 0.258901118\n",
            "Epoch: 0038 cost= 0.258277465\n",
            "Epoch: 0039 cost= 0.257917780\n",
            "Epoch: 0040 cost= 0.257471154\n",
            "Epoch: 0041 cost= 0.257123136\n",
            "Epoch: 0042 cost= 0.256745982\n",
            "Epoch: 0043 cost= 0.256354665\n",
            "Epoch: 0044 cost= 0.255915168\n",
            "Epoch: 0045 cost= 0.255375193\n",
            "Epoch: 0046 cost= 0.254947908\n",
            "Epoch: 0047 cost= 0.254630701\n",
            "Epoch: 0048 cost= 0.254413310\n",
            "Epoch: 0049 cost= 0.254207503\n",
            "Epoch: 0050 cost= 0.253776079\n",
            "Epoch: 0051 cost= 0.253454067\n",
            "Epoch: 0052 cost= 0.253083367\n",
            "Epoch: 0053 cost= 0.252668826\n",
            "Epoch: 0054 cost= 0.252601472\n",
            "Epoch: 0055 cost= 0.252382994\n",
            "Epoch: 0056 cost= 0.251773174\n",
            "Epoch: 0057 cost= 0.251569622\n",
            "Epoch: 0058 cost= 0.251575778\n",
            "Epoch: 0059 cost= 0.251230026\n",
            "Epoch: 0060 cost= 0.250797177\n",
            "Epoch: 0061 cost= 0.250412720\n",
            "Epoch: 0062 cost= 0.250485782\n",
            "Epoch: 0063 cost= 0.250178833\n",
            "Epoch: 0064 cost= 0.249895380\n",
            "Epoch: 0065 cost= 0.249722511\n",
            "Epoch: 0066 cost= 0.249283882\n",
            "Epoch: 0067 cost= 0.249100902\n",
            "Epoch: 0068 cost= 0.248910559\n",
            "Epoch: 0069 cost= 0.248726304\n",
            "Epoch: 0070 cost= 0.248521889\n",
            "Epoch: 0071 cost= 0.248438029\n",
            "Epoch: 0072 cost= 0.248189393\n",
            "Epoch: 0073 cost= 0.248038739\n",
            "Epoch: 0074 cost= 0.247788510\n",
            "Epoch: 0075 cost= 0.247460313\n",
            "Epoch: 0076 cost= 0.247420131\n",
            "Epoch: 0077 cost= 0.246997517\n",
            "Epoch: 0078 cost= 0.246950447\n",
            "Epoch: 0079 cost= 0.246788618\n",
            "Epoch: 0080 cost= 0.246630367\n",
            "Epoch: 0081 cost= 0.246357420\n",
            "Epoch: 0082 cost= 0.246367154\n",
            "Epoch: 0083 cost= 0.246090724\n",
            "Epoch: 0084 cost= 0.245867625\n",
            "Epoch: 0085 cost= 0.245744197\n",
            "Epoch: 0086 cost= 0.245491719\n",
            "Epoch: 0087 cost= 0.245387060\n",
            "Epoch: 0088 cost= 0.245387909\n",
            "Epoch: 0089 cost= 0.244962459\n",
            "Epoch: 0090 cost= 0.245059592\n",
            "Epoch: 0091 cost= 0.244791592\n",
            "Epoch: 0092 cost= 0.244568652\n",
            "Epoch: 0093 cost= 0.244562407\n",
            "Epoch: 0094 cost= 0.244021619\n",
            "Epoch: 0095 cost= 0.244357886\n",
            "Epoch: 0096 cost= 0.244053531\n",
            "Epoch: 0097 cost= 0.243756268\n",
            "Epoch: 0098 cost= 0.243720027\n",
            "Epoch: 0099 cost= 0.243726245\n",
            "Epoch: 0100 cost= 0.243425007\n",
            "Optimization Finished!\n",
            "Accuracy: 0.9033333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}